{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó NYC Motor Vehicle Collisions - Advanced AI/ML Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive machine learning and deep learning analysis of NYC motor vehicle collision data.\n",
    "\n",
    "**Key Features:**\n",
    "- üìä Exploratory Data Analysis (EDA)\n",
    "- ü§ñ Multiple ML Models (Random Forest, XGBoost, LightGBM)\n",
    "- üß† Deep Learning with TensorFlow/Keras\n",
    "- üìà Advanced Visualizations\n",
    "- üéØ Feature Importance Analysis\n",
    "- üîÆ Predictive Insights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Machine Learning Libraries\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ML_AVAILABLE = True\n",
    "    print(\"‚úÖ Scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    ML_AVAILABLE = False\n",
    "    print(f\"‚ùå Scikit-learn not available: {e}\")\n",
    "\n",
    "# Advanced ML Libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost imported successfully\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ùå XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM imported successfully\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ùå LightGBM not available\")\n",
    "\n",
    "# Deep Learning Libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    DL_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} imported successfully\")\n",
    "except ImportError:\n",
    "    DL_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "print(f\"\\nüöÄ Setup Complete!\")\n",
    "print(f\"ML Available: {ML_AVAILABLE}\")\n",
    "print(f\"XGBoost Available: {XGB_AVAILABLE}\")\n",
    "print(f\"LightGBM Available: {LGB_AVAILABLE}\")\n",
    "print(f\"Deep Learning Available: {DL_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('Motor_Vehicle_Collisions_-_Crashes.csv')\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Shape: {df.shape}\")\n",
    "    print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found. Please ensure 'Motor_Vehicle_Collisions_-_Crashes.csv' is in the current directory.\")\n",
    "    print(\"üì• You can download it from: https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # Display basic info\n",
    "    print(\"\\nüìã Column Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nüîç First 5 rows:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîß PREPROCESSING DATA...\")\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    data['CRASH_DATE'] = pd.to_datetime(data['CRASH_DATE'], errors='coerce')\n",
    "    data['CRASH_TIME'] = pd.to_datetime(data['CRASH_TIME'], format='%H:%M', errors='coerce')\n",
    "    \n",
    "    # Extract enhanced time features\n",
    "    data['year'] = data['CRASH_DATE'].dt.year\n",
    "    data['month'] = data['CRASH_DATE'].dt.month\n",
    "    data['day_of_week'] = data['CRASH_DATE'].dt.dayofweek\n",
    "    data['hour'] = data['CRASH_TIME'].dt.hour\n",
    "    data['day_name'] = data['CRASH_DATE'].dt.day_name()\n",
    "    data['month_name'] = data['CRASH_DATE'].dt.month_name()\n",
    "    data['quarter'] = data['CRASH_DATE'].dt.quarter\n",
    "    \n",
    "    # Enhanced binary features\n",
    "    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    data['is_rush_hour'] = ((data['hour'].between(7, 9)) | (data['hour'].between(17, 19))).astype(int)\n",
    "    data['is_night'] = ((data['hour'] >= 22) | (data['hour'] <= 5)).astype(int)\n",
    "    data['is_holiday_season'] = data['month'].isin([11, 12, 1]).astype(int)\n",
    "    \n",
    "    # Calculate casualties\n",
    "    injury_cols = [col for col in data.columns if 'INJURED' in col]\n",
    "    killed_cols = [col for col in data.columns if 'KILLED' in col]\n",
    "    \n",
    "    data['total_injured'] = data[injury_cols].sum(axis=1, skipna=True)\n",
    "    data['total_killed'] = data[killed_cols].sum(axis=1, skipna=True)\n",
    "    data['total_casualties'] = data['total_injured'] + data['total_killed']\n",
    "    \n",
    "    # Enhanced severity classification\n",
    "    data['is_serious'] = ((data['total_killed'] > 0) | (data['total_injured'] >= 2)).astype(int)\n",
    "    \n",
    "    # Enhanced risk scoring\n",
    "    data['risk_score'] = (\n",
    "        data['total_killed'] * 10 + \n",
    "        data['total_injured'] * 3 + \n",
    "        data['is_rush_hour'] * 2 + \n",
    "        data['is_night'] * 1.5 + \n",
    "        data['is_weekend'] * 1.2 +\n",
    "        data['is_holiday_season'] * 1.3\n",
    "    )\n",
    "    \n",
    "    # Remove rows with missing critical data\n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna(subset=['CRASH_DATE', 'hour'])\n",
    "    final_rows = len(data)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing complete!\")\n",
    "    print(f\"üìä Removed {initial_rows - final_rows:,} rows with missing critical data\")\n",
    "    print(f\"üìä Final dataset: {final_rows:,} rows\")\n",
    "    print(f\"üìà Serious Accidents: {data['is_serious'].sum():,} ({data['is_serious'].mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üìä EXPLORATORY DATA ANALYSIS\")\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('NYC Motor Vehicle Collisions - Temporal Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Hourly Distribution\n",
    "    hourly_data = data.groupby('hour').size()\n",
    "    axes[0, 0].bar(hourly_data.index, hourly_data.values, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    axes[0, 0].set_title('Accidents by Hour of Day', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Hour')\n",
    "    axes[0, 0].set_ylabel('Number of Accidents')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Daily Distribution\n",
    "    daily_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    daily_data = data['day_name'].value_counts().reindex(daily_order)\n",
    "    axes[0, 1].bar(range(len(daily_data)), daily_data.values, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "    axes[0, 1].set_title('Accidents by Day of Week', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Day of Week')\n",
    "    axes[0, 1].set_ylabel('Number of Accidents')\n",
    "    axes[0, 1].set_xticks(range(len(daily_data)))\n",
    "    axes[0, 1].set_xticklabels([day[:3] for day in daily_data.index], rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Monthly Distribution\n",
    "    monthly_data = data.groupby('month').size()\n",
    "    axes[0, 2].plot(monthly_data.index, monthly_data.values, marker='o', linewidth=3, markersize=8, color='green')\n",
    "    axes[0, 2].set_title('Accidents by Month', fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Month')\n",
    "    axes[0, 2].set_ylabel('Number of Accidents')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].set_xticks(range(1, 13))\n",
    "    \n",
    "    # 4. Risk Score Distribution\n",
    "    axes[1, 0].hist(data['risk_score'], bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Risk Score Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Risk Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Casualty Analysis\n",
    "    casualty_data = data['total_casualties'].value_counts().head(10)\n",
    "    axes[1, 1].bar(range(len(casualty_data)), casualty_data.values, color='red', alpha=0.7)\n",
    "    axes[1, 1].set_title('Casualty Distribution', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Number of Casualties')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_xticks(range(len(casualty_data)))\n",
    "    axes[1, 1].set_xticklabels(casualty_data.index)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Serious vs Non-serious\n",
    "    serious_counts = data['is_serious'].value_counts()\n",
    "    axes[1, 2].pie(serious_counts.values, labels=['Non-serious', 'Serious'], autopct='%1.1f%%', \n",
    "                   colors=['lightgreen', 'red'], startangle=90)\n",
    "    axes[1, 2].set_title('Accident Severity Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary insights\n",
    "    print(\"\\nüîç KEY INSIGHTS:\")\n",
    "    print(f\"‚Ä¢ Peak accident hour: {hourly_data.idxmax()}:00 with {hourly_data.max():,} accidents\")\n",
    "    print(f\"‚Ä¢ Safest hour: {hourly_data.idxmin()}:00 with {hourly_data.min():,} accidents\")\n",
    "    print(f\"‚Ä¢ Average risk score: {data['risk_score'].mean():.2f}\")\n",
    "    print(f\"‚Ä¢ Total casualties: {data['total_casualties'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and ML_AVAILABLE:\n",
    "    print(\"ü§ñ TRAINING MACHINE LEARNING MODELS\")\n",
    "    \n",
    "    # Select features for ML\n",
    "    feature_columns = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour', \n",
    "                      'is_night', 'is_holiday_season']\n",
    "    \n",
    "    # Add coordinates if available\n",
    "    if 'LATITUDE' in data.columns and 'LONGITUDE' in data.columns:\n",
    "        data['LATITUDE'] = pd.to_numeric(data['LATITUDE'], errors='coerce')\n",
    "        data['LONGITUDE'] = pd.to_numeric(data['LONGITUDE'], errors='coerce')\n",
    "        feature_columns.extend(['LATITUDE', 'LONGITUDE'])\n",
    "    \n",
    "    # Prepare the dataset\n",
    "    ml_data = data[feature_columns + ['is_serious']].dropna()\n",
    "    print(f\"üìä ML Dataset shape: {ml_data.shape}\")\n",
    "    \n",
    "    if len(ml_data) >= 1000:\n",
    "        # Prepare features and target\n",
    "        X = ml_data[feature_columns]\n",
    "        y = ml_data['is_serious']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "        \n",
    "        # Initialize models dictionary\n",
    "        models = {}\n",
    "        model_scores = {}\n",
    "        \n",
    "        # 1. Random Forest\n",
    "        print(\"\\nüå≤ Training Random Forest...\")\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "        rf_score = roc_auc_score(y_test, rf_pred_proba)\n",
    "        models['Random Forest'] = rf_model\n",
    "        model_scores['Random Forest'] = rf_score\n",
    "        print(f\"   ‚úÖ AUC Score: {rf_score:.4f}\")\n",
    "        \n",
    "        # 2. Gradient Boosting\n",
    "        print(\"üöÄ Training Gradient Boosting...\")\n",
    "        gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            random_state=42\n",
    "        )\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "        gb_score = roc_auc_score(y_test, gb_pred_proba)\n",
    "        models['Gradient Boosting'] = gb_model\n",
    "        model_scores['Gradient Boosting'] = gb_score\n",
    "        print(f\"   ‚úÖ AUC Score: {gb_score:.4f}\")\n",
    "        \n",
    "        # 3. XGBoost (if available)\n",
    "        if XGB_AVAILABLE:\n",
    "            print(\"‚ö° Training XGBoost...\")\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "            xgb_score = roc_auc_score(y_test, xgb_pred_proba)\n",
    "            models['XGBoost'] = xgb_model\n",
    "            model_scores['XGBoost'] = xgb_score\n",
    "            print(f\"   ‚úÖ AUC Score: {xgb_score:.4f}\")\n",
    "        \n",
    "        # 4. LightGBM (if available)\n",
    "        if LGB_AVAILABLE:\n",
    "            print(\"üí° Training LightGBM...\")\n",
    "            lgb_model = lgb.LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbosity=-1\n",
    "            )\n",
    "            lgb_model.fit(X_train, y_train)\n",
    "            lgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "            lgb_score = roc_auc_score(y_test, lgb_pred_proba)\n",
    "            models['LightGBM'] = lgb_model\n",
    "            model_scores['LightGBM'] = lgb_score\n",
    "            print(f\"   ‚úÖ AUC Score: {lgb_score:.4f}\")\n",
    "        \n",
    "        # Best model\n",
    "        best_model_name = max(model_scores.keys(), key=lambda k: model_scores[k])\n",
    "        best_model = models[best_model_name]\n",
    "        print(f\"\\nüèÜ Best Model: {best_model_name} (AUC: {model_scores[best_model_name]:.4f})\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Insufficient data for ML training\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå ML libraries not available or no data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DL_AVAILABLE and 'X_train' in locals():\n",
    "    print(\"üß† TRAINING DEEP LEARNING MODEL\")\n",
    "    \n",
    "    # Scale features for neural network\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create neural network model\n",
    "    dl_model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    dl_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"üèóÔ∏è Model Architecture:\")\n",
    "    dl_model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nüöÄ Training Neural Network...\")\n",
    "    history = dl_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        batch_size=256,\n",
    "        epochs=30,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    dl_pred_proba = dl_model.predict(X_test_scaled).flatten()\n",
    "    dl_score = roc_auc_score(y_test, dl_pred_proba)\n",
    "    \n",
    "    print(f\"\\nüéØ Deep Learning Model AUC Score: {dl_score:.4f}\")\n",
    "    \n",
    "    # Add to model comparison\n",
    "    if 'model_scores' in locals():\n",
    "        model_scores['Deep Learning'] = dl_score\n",
    "        models['Deep Learning'] = dl_model\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Deep Learning not available or ML training failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model_scores' in locals() and model_scores:\n",
    "    print(\"üìä MODEL COMPARISON\")\n",
    "    \n",
    "    # Create comparison chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    model_names = list(model_scores.keys())\n",
    "    scores = list(model_scores.values())\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple'][:len(model_names)]\n",
    "    \n",
    "    bars = plt.bar(model_names, scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "    plt.title('Model Performance Comparison (AUC Scores)', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Models', fontsize=12)\n",
    "    plt.ylabel('AUC Score', fontsize=12)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Final model ranking\n",
    "    print(\"\\nüèÜ FINAL MODEL RANKING:\")\n",
    "    ranked_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (model_name, score) in enumerate(ranked_models, 1):\n",
    "        medal = ['ü•á', 'ü•à', 'ü•â'][i-1] if i <= 3 else f'{i}.'\n",
    "        print(f\"   {medal} {model_name}: {score:.4f}\")\n",
    "    \n",
    "    # Feature importance for best tree-based model\n",
    "    best_model_name = ranked_models[0][0]\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(f\"\\nüîç FEATURE IMPORTANCE ({best_model_name}):\")\n",
    "        \n",
    "        feature_importance = best_model.feature_importances_\n",
    "        \n",
    "        # Create DataFrame for easy manipulation\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': feature_importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Display feature importance\n",
    "        for i, (_, row) in enumerate(importance_df.iterrows(), 1):\n",
    "            print(f\"   {i:2d}. {row['Feature']:15s}: {row['Importance']:.4f}\")\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(importance_df)), importance_df['Importance'], color='steelblue', alpha=0.8)\n",
    "        plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'Feature Importance - {best_model_name}')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No models were successfully trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risk Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"‚ö†Ô∏è RISK ANALYSIS AND INSIGHTS\")\n",
    "    \n",
    "    # Risk analysis by hour\n",
    "    hourly_risk = data.groupby('hour').agg({\n",
    "        'is_serious': ['count', 'mean'],\n",
    "        'risk_score': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    hourly_risk.columns = ['Total_Accidents', 'Serious_Rate', 'Avg_Risk_Score']\n",
    "    hourly_risk = hourly_risk.reset_index()\n",
    "    \n",
    "    # Identify high-risk hours\n",
    "    high_risk_threshold = hourly_risk['Serious_Rate'].quantile(0.75)\n",
    "    high_risk_hours = hourly_risk[hourly_risk['Serious_Rate'] >= high_risk_threshold]['hour'].tolist()\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è HIGH-RISK HOURS (Top 25% by serious rate):\")\n",
    "    for hour in sorted(high_risk_hours):\n",
    "        rate = hourly_risk[hourly_risk['hour'] == hour]['Serious_Rate'].iloc[0]\n",
    "        total = hourly_risk[hourly_risk['hour'] == hour]['Total_Accidents'].iloc[0]\n",
    "        print(f\"   üïê {hour:02d}:00 - Serious Rate: {rate:.1%}, Total: {total:,}\")\n",
    "    \n",
    "    # Risk by conditions\n",
    "    print(f\"\\nüìä RISK BY CONDITIONS:\")\n",
    "    conditions = {\n",
    "        'Weekend vs Weekday': (data['is_weekend'] == 1, data['is_weekend'] == 0),\n",
    "        'Rush Hour vs Normal': (data['is_rush_hour'] == 1, data['is_rush_hour'] == 0),\n",
    "        'Night vs Day': (data['is_night'] == 1, data['is_night'] == 0),\n",
    "        'Holiday Season vs Normal': (data['is_holiday_season'] == 1, data['is_holiday_season'] == 0)\n",
    "    }\n",
    "    \n",
    "    for condition_name, (high_risk_mask, low_risk_mask) in conditions.items():\n",
    "        high_risk_rate = data[high_risk_mask]['is_serious'].mean()\n",
    "        low_risk_rate = data[low_risk_mask]['is_serious'].mean()\n",
    "        ratio = high_risk_rate / low_risk_rate if low_risk_rate > 0 else float('inf')\n",
    "        print(f\"   {condition_name}: {high_risk_rate:.1%} vs {low_risk_rate:.1%} (Ratio: {ratio:.2f}x)\")\n",
    "    \n",
    "    # Top risk factors visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Subplot 1: Hourly serious rate\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(hourly_risk['hour'], hourly_risk['Serious_Rate'], marker='o', linewidth=2, markersize=6)\n",
    "    plt.title('Serious Accident Rate by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Serious Rate')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Risk by day of week\n",
    "    plt.subplot(1, 3, 2)\n",
    "    daily_risk = data.groupby('day_name')['is_serious'].mean().reindex(\n",
    "        ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    )\n",
    "    plt.bar(range(len(daily_risk)), daily_risk.values, color='coral', alpha=0.7)\n",
    "    plt.title('Serious Accident Rate by Day')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Serious Rate')\n",
    "    plt.xticks(range(len(daily_risk)), [day[:3] for day in daily_risk.index], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Risk by month\n",
    "    plt.subplot(1, 3, 3)\n",
    "    monthly_risk = data.groupby('month')['is_serious'].mean()\n",
    "    plt.plot(monthly_risk.index, monthly_risk.values, marker='s', linewidth=2, markersize=6, color='green')\n",
    "    plt.title('Serious Accident Rate by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Serious Rate')\n",
    "    plt.xticks(range(1, 13))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Key insights summary\n",
    "    print(f\"\\nüîç KEY INSIGHTS SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Most dangerous hour: {hourly_risk.loc[hourly_risk['Serious_Rate'].idxmax(), 'hour']}:00\")\n",
    "    print(f\"   ‚Ä¢ Safest hour: {hourly_risk.loc[hourly_risk['Serious_Rate'].idxmin(), 'hour']}:00\")\n",
    "    print(f\"   ‚Ä¢ Peak accident day: {daily_risk.idxmax()}\")\n",
    "    print(f\"   ‚Ä¢ Safest day: {daily_risk.idxmin()}\")\n",
    "    print(f\"   ‚Ä¢ Most dangerous month: {monthly_risk.idxmax()}\")\n",
    "    print(f\"   ‚Ä¢ Safest month: {monthly_risk.idxmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nüìä ANALYSIS SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Total Records Analyzed: {len(data):,}\")\n",
    "    print(f\"   ‚Ä¢ Total Casualties: {data['total_casualties'].sum():,}\")\n",
    "    print(f\"   ‚Ä¢ Serious Accident Rate: {data['is_serious'].mean():.1%}\")\n",
    "    print(f\"   ‚Ä¢ Average Risk Score: {data['risk_score'].mean():.2f}\")\n",
    "\n",
    "if 'model_scores' in locals() and model_scores:\n",
    "    best_score = max(model_scores.values())\n",
    "    best_model_name = max(model_scores.keys(), key=lambda k: model_scores[k])\n",
    "    print(f\"\\nü§ñ MODEL PERFORMANCE:\")\n",
    "    print(f\"   ‚Ä¢ Best Model: {best_model_name}\")\n",
    "    print(f\"   ‚Ä¢ Best AUC Score: {best_score:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Models Trained: {len(model_scores)}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY RECOMMENDATIONS:\")\n",
    "print(f\"   1. üö¶ Enhanced Traffic Control:\")\n",
    "print(f\"      - Deploy additional enforcement during peak risk hours\")\n",
    "print(f\"      - Implement adaptive traffic signal timing\")\n",
    "print(f\"      - Increase police presence in high-risk areas\")\n",
    "\n",
    "print(f\"\\n   2. üì± Public Safety Campaigns:\")\n",
    "print(f\"      - Target awareness campaigns during high-risk times\")\n",
    "print(f\"      - Promote alternative transportation during peak hours\")\n",
    "print(f\"      - Educate drivers about night-time driving risks\")\n",
    "\n",
    "print(f\"\\n   3. üèóÔ∏è Infrastructure Improvements:\")\n",
    "print(f\"      - Improve lighting in accident-prone areas\")\n",
    "print(f\"      - Install additional safety barriers\")\n",
    "print(f\"      - Optimize road design in high-risk zones\")\n",
    "\n",
    "print(f\"\\n   4. üìä Data-Driven Decisions:\")\n",
    "print(f\"      - Use ML predictions for resource allocation\")\n",
    "print(f\"      - Implement real-time risk monitoring\")\n",
    "print(f\"      - Develop early warning systems\")\n",
    "\n",
    "print(f\"\\n‚úÖ NEXT STEPS:\")\n",
    "print(f\"   ‚Ä¢ Deploy best model in production for real-time predictions\")\n",
    "print(f\"   ‚Ä¢ Integrate with city traffic management systems\")\n",
    "print(f\"   ‚Ä¢ Continuously retrain models with new data\")\n",
    "print(f\"   ‚Ä¢ Validate predictions against actual outcomes\")\n",
    "\n",
    "print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "print(f\"This analysis provides actionable insights for improving traffic safety in NYC.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}